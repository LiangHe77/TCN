{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCausalConv1d(nn.Module):\n",
    "    def __init__(self,channels,dilation=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(channels,channels,kernel_size=2,stride=1,dilation=dilation,padding=0)\n",
    "        \n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules:\n",
    "            if isinstance(m,nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        output = self.conv(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels,out_channels,kernel_size=2,stride=1,padding=1)\n",
    "        \n",
    "    def init_weights_for_test(self):\n",
    "        for m in self.modules:\n",
    "            if isinstance(m,nn.Conv1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                    \n",
    "    def forward(self,x):\n",
    "        output = self.conv(x)\n",
    "        return output[:,:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,res_channels,skip_channels,dilation):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dilated = DilatedCausalConv1d(res_channels,dilation=dilation)\n",
    "        self.conv_res = nn.Conv1d(res_channels,res_channels,1)\n",
    "        self.conv_skip = nn.Conv1d(res_channels,skip_channels,1)\n",
    "        \n",
    "        self.gate_tanh = nn.Tanh()\n",
    "        self.gate_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,skip_size):\n",
    "        output = self.dilated(x)\n",
    "        \n",
    "        gated_tanh = self.gate_tanh(output)\n",
    "        gated_sigmoid = self.gate_sigmoid(output)\n",
    "        gated = gated_tanh*gated_sigmoid\n",
    "        \n",
    "        output = self.conv(gated)\n",
    "        input_cut = x[:, :, -output.size(2):]\n",
    "        output += input_cut\n",
    "        \n",
    "        skip = self.conv_skip(gated)\n",
    "        skip = skip[:, :, -skip_size:]\n",
    "\n",
    "        return output, skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualStack(torch.nn.Module):\n",
    "    def __init__(self, layer_size, stack_size, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Stack residual blocks by layer and stack size\n",
    "        :param layer_size: integer, 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "        :param stack_size: integer, 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        :param res_channels: number of residual channel for input, output\n",
    "        :param skip_channels: number of skip channel for output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(ResidualStack, self).__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.stack_size = stack_size\n",
    "\n",
    "        self.res_blocks = self.stack_res_block(res_channels, skip_channels)\n",
    "\n",
    "    def _residual_block(res_channels, skip_channels, dilation):\n",
    "        block = ResidualBlock(res_channels, skip_channels, dilation)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            block = torch.nn.DataParallel(block)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            block.cuda()\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def build_dilations(self):\n",
    "        dilations = []\n",
    "\n",
    "        # 5 = stack[layer1, layer2, layer3, layer4, layer5]\n",
    "        for s in range(0, self.stack_size):\n",
    "            # 10 = layer[dilation=1, dilation=2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "            for l in range(0, self.layer_size):\n",
    "                dilations.append(2 ** l)\n",
    "\n",
    "        return dilations\n",
    "    \n",
    "    def stack_res_block(self, res_channels, skip_channels):\n",
    "        \"\"\"\n",
    "        Prepare dilated convolution blocks by layer and stack size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res_blocks = []\n",
    "        dilations = self.build_dilations()\n",
    "\n",
    "        for dilation in dilations:\n",
    "            block = self._residual_block(res_channels, skip_channels, dilation)\n",
    "            res_blocks.append(block)\n",
    "\n",
    "        return res_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensNet(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        The last network of WaveNet\n",
    "        :param channels: number of channels for input and output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 1)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.relu(x)\n",
    "        output = self.conv1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_depth=256, res_channels=32, skip_channels=512, dilation_depth=10, n_repeat=5):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.dilations = [2**i for i in range(dilation_depth)] * n_repeat\n",
    "        self.main = nn.ModuleList([ResidualBlock(res_channels,skip_channels,dilation) for dilation in self.dilations])\n",
    "        self.pre = nn.Embedding(in_depth, res_channels)\n",
    "        #self.pre_conv = CausalConv1d(in_channels=res_channels, out_channels=res_channels)\n",
    "        self.post = nn.Sequential(nn.ReLU(),\n",
    "                                  nn.Conv1d(skip_channels,skip_channels,1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv1d(skip_channels,in_depth,1))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        outputs = self.preprocess(inputs)\n",
    "        skip_connections = []\n",
    "        \n",
    "        for layer in self.main:\n",
    "            outputs,skip = layer(outputs)\n",
    "            skip_connections.append(skip)\n",
    "            \n",
    "        outputs = sum([s[:,:,-outputs.size(2):] for s in skip_connections])\n",
    "        outputs = self.post(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def preprocess(self,inputs):\n",
    "        out = self.pre(inputs).transpose(1,2)\n",
    "        #out = self.pre_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
